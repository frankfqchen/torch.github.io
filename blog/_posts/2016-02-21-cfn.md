---
layout: post
title: Collaborative Filtering with Neural Networks
comments: True
author: florian-strub
excerpt: Collaborative Filtering methods are a key algorithm for Recommender Systems. They are used by major companies such as Netflix, Amazon or Spotify to suggest items to users.  While Neural Networks have tremendous success in image and speech recognition, Collaborative Filtering received less attention as it deals with sparse inputs/targets. After a quick introduction to Recommendation Systems, we show how to implement Autoencoders to perform Collaborative Filtering.
picture: http://insidebigdata.com/wp-content/uploads/2014/06/Humor_recommender.jpg
---

<!---# Recurrent Model of Visual Attention-->

Deep Learning has well-known state-of-the arts results in Computer Vision, Speech Recognition or Natural Language Processing.
This post aims at using Neural Networks on a different topic : Recommender systems and Collaborative Filtering. 
It turns out that Neural Networks also perform among the best Collaborative Fitering algorithms. 

#Introduction#

Recommendation systems are a subclass of algorithms that seek to predict the rating that a user would give to an item.
A good recommendation system may dramatically increase the amount of sales of a firm or retain customers. For instance, 80% of movies watched on Netflix come from the recommender system of the company

One of the most successful approach of Recommendation systems is to use the explicit feedback of users to build a Matrix of ratings. This technique is known as Collaborative Filtering.

For instance, if we gather the feedback of all users into a matrix, we obtain a sparse matrix of ratings. Note that sparsity is induced by unknown values.

![ratings](http://florian-strub.com/tmp/initialMatrix.png)

In this case, should we recommend Bob to watch _The Exorcist_ or _Titanic_?

Collaborative Filtering aims at estimating the ratings a user would have given to other movies by using the ratings of all the other users.
Inherent features of users or items such as the genre, age, actors or some descriptions are _not_ take into account. 
The goal is then to turn the previous sparse matrix of ratings into a dense one:

![ratings](http://florian-strub.com/tmp/finalMatrix.png)

Therefore, the Recommender System will advise Bob to watch _The Exorcist_. 

If Bob eventually rates _The Exorcist_, the matrix is re-estimated and so on... 
In practice, the matrix of ratings is very sparse. IMDB currently inventors around 1,7 millions movies while users rarely rate more than a few hundreds movies. 

For more details about Collaborative Filtering, I recommend the [following paper](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf) that provide a great insight about the major challenges (Latent factors, Cold-Sart Problem, Hybrid Systems etc.)

In this post, we will introduce a Collaborative Filtering approach based on Stacked Denoising Autoencoders.

# Understanding the model #

As a short reminder, Autoencoders are unsupervised Networks where the output of the Network aims at reconstructing the initial input. 
The Network is constrained to use narrow hidden layers, forcing a dimensionality reduction on the data. 

In a Collaborative Filtering task, the goal is to turn the sparse matrix of ratings into a dense one. 
To do so, we are going to decompose the matrix by either its lines or columns. 
The Autoencoder is then fed these sparse vectors, its ouput will be our line/column estimate.

 ![ratings](http://florian-strub.com/tmp/UV.png)

Note that the input/output size will depend on the matrix dimension.

Before implemeting the Autoencoder, we need to define how to handle unknwon values. 

In the forward pass, a naive but efficient approach is to turn unkwown values into zero values.
This is strictly equivalent to remove the neuron from the network. In other words, unknown values are ignored.    

In the backward pass, the same idea can be applied. When no rating exist, the error is set to zero as it cannot be computed.

 ![ratings](http://florian-strub.com/tmp/training1-small.png)

Beware that for real datasets, more than 95% of the ratings are missing. Thus, the bottleneck of the autoencoder is often greater than the input vector!
The autoencoder is also divert from its original purpose. It does not aim to reconstruct the input, it aims at predicting it.


# Implementation #

Let's start the interesting part!

First, we need to install the nnsparse package to have basic tools to handle sparse data.

```
luarocks install nnsparse
```


## The data ##
We are going to work on a small but classic dataset called [movieLens-1M](http://grouplens.org/datasets/movielens/). 
This dataset contains 1 million ratings from 1 to 5 given by 6040 users on 3700 movies. 

```lua

   train, test = {}, {}

   -- Step 1 : Load file
   local ratesfile = io.open("ml-1m/ratings.dat, "r")

   -- Step 2 : Retrieve ratings
   for line in ratesfile:lines() do

	  -- parse the file by using regex
      local userIdStr, movieIdStr, ratingStr, _ = line:match('(%d+)::(%d+)::(%d%.?%d?)::(%d+)')

      local userId  = tonumber(userIdStr)
      local itemId  = tonumber(movieIdStr)
      local rating  = tonumber(ratingStr)

	  -- normalize the rating between [-1, 1]
      rating = (rating-3)/2

	  -- we are going to autoencode the item with a training ratio of 0.9
	  if torch.uniform() < 0.9 do
		  train[itemId] = train[itemId] or nnsparse.DynamicSparseTensor()
		  train[itemId]:append(torch.Tensor{userId, rating})
	  else
		  test[itemId] = test[itemId] or nnsparse.DynamicSparseTensor()
		  test[itemId]:append(torch.Tensor{userId, rating})
	  end 

   end

```

Given the toy matrix in the first section, the full dataset would be encoded this way:


|train["Exorcist" = 1]             | train["Inception"= 2]           | train["Psycho"= 3]               |train["Titanic"= 4]               |
|----------------------------------|---------------------------------|----------------------------------|----------------------------------|
| 3 : 5                            | 1 : 3                           | 1 : 5                            | 2 : 5                            |
| 4 : 2                            | 2 : 5                           | 3 : 4                            | 4 : 4                            |
|                                  |                                 | 4 : 1                            |                                  |


## The Network ##
We will first implement a shallow Autoencoder with sparse inputs. Deeper Autoencoder can be used, but the final results will only be increased by a small margin.
So let keep thing simple!

- On CPU:

```lua
local network = nn.Sequential()
network:add(nnsparse.SparseLinearBatch(6040, 500)) --There are 6040 users in movieLens-1M
network:add(nn.Tanh())
network:add(nn.Linear(500, 6040))
network:add(nn.Tanh())
```

- On GPU:

```lua
local network = nn.Sequential()
network:add(nnsparse.Densify(6040)) --GPU does not handle sparse data, so we densify the input on the fly
network:add(nn.Linear(6040, 500))
network:add(nn.Tanh())
network:add(nn.Linear(500, 6040))
network:add(nn.Tanh())
network:cuda()
```

## The Loss ##








## References
1. *Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, (8), 30-37., 
